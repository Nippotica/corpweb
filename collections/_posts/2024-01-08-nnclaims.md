---
layout: post
title: "Who'd Claim?"
date: 2024-01-08
authors: ["Mike Vance"]
categories: ["Mathnote", "Technote"]
description: Predicting life insurance claims with neural networks
thumbnail: "/assets/images/gen/blog/nnclaims2.webp"

---

> Neural networks can help predict life insurance claims. 

These networks capture complex relationships in data that traditional actuarial models may miss. Let's explore how a simple neural network works for this use case.

### Neural Network Structure

A neural network consists of layers of interconnected nodes (neurons). The basic structure includes:
- **Input Layer**: Takes input features (e.g., age, gender, health status).
- **Hidden Layers**: Perform computations to detect patterns.
- **Output Layer**: Produces the prediction (e.g., probability of a claim).

### Algebraic Formulations

#### Input and Weights

Each neuron in a hidden layer receives input \\( x_i \\) (features) and has associated weights \\( w_i \\). The net input \\( z \\) to a neuron is given by:

\\[ z = \sum_{i=1}^{n} w_i x_i + b \\]

where \\( b \\) is the bias term.

#### Activation Function

The net input \\( z \\) is passed through an activation function \\( f(z) \\) to introduce non-linearity. Common activation functions include the sigmoid, ReLU, and tanh.

- **Sigmoid**: \\( f(z) = \frac{1}{1 + e^{-z}} \\)
- **ReLU**: \\( f(z) = \max(0, z) \\)
- **Tanh**: \\( f(z) = \tanh(z) \\)

#### Output

For a binary classification problem (e.g., predicting whether a claim will be made or not), the output layer often uses the sigmoid activation function. The final output \\( \hat{y} \\) is:

\\[ \hat{y} = \sigma \left( \sum_{j=1}^{m} w_j f(z_j) + b \right) \\]

where \\( \sigma \\) is the sigmoid function, and \\( m \\) is the number of neurons in the previous layer.

### Training the Network

The network is trained using a dataset with known outcomes. The objective is to minimize the loss function, typically the binary cross-entropy loss for classification problems:

\\[ L(y, \hat{y}) = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right] \\]

where \\( y_i \\) is the actual label, \\( \hat{y}_i \\) is the predicted probability, and \\( N \\) is the number of samples.

The optimization is done using algorithms like gradient descent, which updates the weights \\( w_i \\) and biases \\( b \\) iteratively:

\\[ w_i := w_i - \eta \frac{\partial L}{\partial w_i} \\]
\\[ b := b - \eta \frac{\partial L}{\partial b} \\]

where \\( \eta \\) is the learning rate.

### Numerical Example

#### Dataset

Assume we have a simplified dataset with the following features and labels:

| Age | Gender | Health Score | Claim (1/0) |
|-----|--------|--------------|-------------|
| 45  | 0      | 0.8          | 1           |
| 34  | 1      | 0.5          | 0           |
| 50  | 0      | 0.6          | 1           |
| 28  | 1      | 0.9          | 0           |


Age and Health Score are numerical features, and Gender is a binary feature (0 for male, 1 for female).

#### Network Configuration

- **Input Layer**: 3 neurons (Age, Gender, Health Score)
- **Hidden Layer**: 2 neurons, ReLU activation
- **Output Layer**: 1 neuron, Sigmoid activation



#### Input to Hidden Layer

   Assume initial weights \\( w_{11}, w_{12}, w_{21}, w_{22}, w_{31}, w_{32} \\) and biases \\( b_1, b_2 \\):

   \\[ z_1 = w_{11} \cdot \text{Age} + w_{21} \cdot \text{Gender} + w_{31} \cdot \text{Health Score} + b_1 \\]
   \\[ z_2 = w_{12} \cdot \text{Age} + w_{22} \cdot \text{Gender} + w_{32} \cdot \text{Health Score} + b_2 \\]

   Using ReLU activation:

   \\[ a_1 = \max(0, z_1) \\]
   \\[ a_2 = \max(0, z_2) \\]

#### Hidden to Output Layer

   Assume weights \\( w_{1o}, w_{2o} \\) and bias \\( b_o \\):

   \\[ z_o = w_{1o} \cdot a_1 + w_{2o} \cdot a_2 + b_o \\]
   Using sigmoid activation:

   \\[ \hat{y} = \frac{1}{1 + e^{-z_o}} \\]

#### Example Calculation

For the first row in the dataset (Age=45, Gender=0, Health Score=0.8):

Assume the following weights and biases for simplicity:
\\[ w_{11} = 0.1, w_{21} = -0.2, w_{31} = 0.3 \\]
\\[ w_{12} = 0.2, w_{22} = 0.1, w_{32} = -0.3 \\]
\\[ b_1 = 0.1, b_2 = -0.1 \\]
\\[ w_{1o} = 0.4, w_{2o} = -0.5, b_o = 0.2 \\]

Calculate the hidden layer outputs:
\\[ z_1 = 0.1 \cdot 45 + (-0.2) \cdot 0 + 0.3 \cdot 0.8 + 0.1 = 4.14 \\]
\\[ z_2 = 0.2 \cdot 45 + 0.1 \cdot 0 + (-0.3) \cdot 0.8 - 0.1 = 8.76 \\]

\\[ a_1 = \max(0, 4.14) = 4.14 \\]
\\[ a_2 = \max(0, 8.76) = 8.76 \\]

Calculate the output:
\\[ z_o = 0.4 \cdot 4.14 + (-0.5) \cdot 8.76 + 0.2 = -0.414 \\]
\\[ \hat{y} = \frac{1}{1 + e^{0.414}} \approx 0.397 \\]

The predicted probability of a claim for this input is approximately 0.397, or almost 40%.


Neural networks provide a new approach to predicting life insurance claims by capturing complex relationships in the data. Through proper training and tuning, these models can significantly enhance decision-making processes in the insurance industry.
